<!-- start of page body -->
<td width="80%" valign="TOP">
  <table width="100%" border="0" cellspacing="2" cellpadding="5">
    <tr><td>
    <TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0> <TR> <TD BGCOLOR=#000000 HEIGHT=1><TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0><TR><TD></TD></TR></TABLE></TD> </TR> </TABLE>
    <TABLE BORDER=0 WIDTH=100% CELLPADDING=2 CELLSPACING=0>
      <TR><td bgcolor="#EEEED4">
      <b>TRANSFORM: flexible voice synthesis through articulatory voice transformation</b>
      </TD></tr>
    </TABLE>
    <TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0> <TR> <TD BGCOLOR=#000000 HEIGHT=1><TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0><TR><TD></TD></TR></TABLE></TD> </TR> </TABLE>
    <table width="100%" border="0" cellpadding="10"><tr><td>
    <H3>TRANSFORM</H3>
    <P>
    Contact: <a href="http://www.cs.cmu.edu/~awb/">Alan W Black</a><br>
    or <a href="http://www.cs.cmu.edu/~atoth/">Arthur Toth</a><br>
    <P>     
We have always wanted our machines to talk to us, but most people
have strong preferences for particular voices.  Current techniques in
speech synthesis can build voices that sound very close to the
original speaker, capturing the style, manner and articulation of the
source voice.  However such systems require many hours of carefully
recorded speech and expert tuning to reach an acceptable level of
quality.
    <P>     
An exciting new alternative method for building synthetic voices is
voice transformation.  Here we use an exsisting recorded database and
convert it to a target voice using as little as 10-20 sentences.
These techniques offer the potential to make speech synthesizers talk
in whatever voice we desire, with significantly less effort required
than previous techniques.
    <P>     
This project offers a new direction in voice transformation.  Current
transformation techniques concentrate on a spectral mapping of the
voice, i.e. converting the properties of the speech signal.  Instead
we can use the underlying positions of the vocal tract articulators
(i.e.  the position of the teeth, tongue, lips, velum) which give rise
to the spectral output of the voice.
    <P>     
Using new statistical modeling techniques we can successfully predict
the positions of a speaker's articulators from the speech signal.  Then
in the virtual vocal tract domain map between speakers and regenerate 
the speech for the target voice.
    <P>     
This work enables the easy construction of new synthetic voices
allowing personalization of speech output.  It increases our knowledge
of the speech generation process and characterizes what make a voice
personal.

    <TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0> <TR> <TD BGCOLOR=#000000 HEIGHT=1><TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0><TR><TD></TD></TR></TABLE></TD> </TR> </TABLE>
    <TABLE BORDER=0 WIDTH=100% CELLPADDING=2 CELLSPACING=0>
      <TR><td bgcolor="#EEEED4">
      <b>Voice Transformation Publications</b>
      </TD></tr>
    </TABLE>
    <TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0> <TR> <TD BGCOLOR=#000000 HEIGHT=1><TABLE BORDER=0 WIDTH=100% CELLPADDING=0 CELLSPACING=0><TR><TD></TD></TR></TABLE></TD> </TR> </TABLE>

<ul>
<LI>Toth, A., and Black, A., (2005)
<em>Cross-Speaker Articulatory Position Data for Phonetic Feature Prediction</em>
Interspeech 2005, Lisbon, Portugal.
(<A href="http://www.cs.cmu.edu/~awb/papers/is2005/IS051600.PDF">pdf</A>)
</LI>
<LI>Toda, T., Black, A., and Tokuda, K. (2005)
<em>Spectral Conversion Based on Maximum Likelihood Estimation
Considering Global Variance of Converted Parameter</em>
ICASSP, Philadelphia, Pennsylvania.
(<A href="http://www.cs.cmu.edu/~awb/papers/icassp2005/0100009.pdf">pdf</A>)
</LI>
<LI>Toda, T., and Black, A., and Tokuda, K. (2004)
<em>
Acoustic-to-Articulatory Inversion Mapping with Gaussian
Mixture Model</em>,
ICSLP2004, Jeju, Korea,
(<A href="http://www.cs.cmu.edu/~awb/papers/ICSLP2004/WeB601o.6_p1180.pdf">pdf</A>)
</LI>
<LI>Toda, T., and Black, A., and Tokuda, K. (2004)
<em>
Acoustic-to-Articulatory Inversion Mapping with Gaussian
Mixture Model</em>,
ICSLP2004, Jeju, Korea,
(<A href="http://www.cs.cmu.edu/~awb/papers/ICSLP2004/WeB601o.6_p1180.pdf">pdf</A>)
</LI>
</ul>

  </table>
</td>
